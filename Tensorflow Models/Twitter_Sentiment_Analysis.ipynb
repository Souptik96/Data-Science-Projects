{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9d2ff1cc",
      "metadata": {
        "id": "9d2ff1cc"
      },
      "source": [
        "#### The \"Twitter Sentiment Extraction\" project aims to leverage the power of Natural Language Processing (NLP) and Machine Learning to analyze Twitter data and extract sentiments from tweets. Twitter, being a prominent social media platform, is a treasure trove of opinions, emotions, and insights expressed by users worldwide. Understanding the sentiments and emotions embedded in these short messages can provide valuable insights for businesses, marketers, researchers, and policymakers.\n",
        "\n",
        "#### The primary goal of this project is to build a sentiment analysis model capable of determining whether a tweet carries a positive, negative, or neutral sentiment. Sentiment analysis is a subfield of NLP that focuses on understanding and categorizing the emotions conveyed through textual data.\n",
        "\n",
        "#### However, we don't stop at just determining the sentiment. This project goes beyond standard sentiment analysis by taking it a step further â€“ extracting the most critical words or phrases contributing to the sentiment of a tweet. By doing so, we gain deeper insights into why a particular tweet is classified with a specific sentiment, enabling us to identify what aspects or topics are driving the emotions expressed by Twitter users."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "267d9c2d",
      "metadata": {
        "id": "267d9c2d"
      },
      "source": [
        "### Key Objectives:\n",
        "\n",
        "- Sentiment Analysis: Develop an accurate sentiment analysis model that can accurately categorize tweets as positive, negative, or neutral based on their textual content.\n",
        "\n",
        "- Sentiment Extraction: Go beyond traditional sentiment analysis by extracting the most important words or phrases that contribute to the overall sentiment of each tweet.\n",
        "\n",
        "- Data Exploration: Understand the characteristics of the Twitter dataset, explore the distribution of sentiments, and identify patterns or trends in the data.\n",
        "\n",
        "- Model Evaluation: Evaluate the performance of the sentiment analysis model using various metrics, ensuring its effectiveness in predicting sentiments.\n",
        "\n",
        "- Interpretation and Visualization: Visualize the extracted words or phrases in an interpretable and insightful manner to gain a deeper understanding of the sentiments expressed in the tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1691be",
      "metadata": {
        "id": "5c1691be"
      },
      "source": [
        "## Importing Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "id": "lopibQyE144Q"
      },
      "id": "lopibQyE144Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/root/.kaggle')"
      ],
      "metadata": {
        "id": "r-jMYU_lqVYh"
      },
      "id": "r-jMYU_lqVYh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv kaggle.json ~/.kaggle"
      ],
      "metadata": {
        "id": "UAECFEakxTGf"
      },
      "id": "UAECFEakxTGf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "qUH6x3p60kSW"
      },
      "id": "qUH6x3p60kSW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c tweet-sentiment-extraction"
      ],
      "metadata": {
        "id": "UoI2yUkH0mZm"
      },
      "id": "UoI2yUkH0mZm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4598e42",
      "metadata": {
        "id": "f4598e42"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from collections import Counter\n",
        "\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "import spacy\n",
        "import random\n",
        "from spacy.util import compounding\n",
        "from spacy.util import minibatch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16be163c",
      "metadata": {
        "id": "16be163c"
      },
      "source": [
        "## Importing Data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tweet-sentiment-extraction.zip -d /content/"
      ],
      "metadata": {
        "id": "iEHm_EJd5x2-"
      },
      "id": "iEHm_EJd5x2-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1907d6",
      "metadata": {
        "id": "5c1907d6"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "ss = pd.read_csv('/content/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6704b810",
      "metadata": {
        "id": "6704b810"
      },
      "source": [
        "#### Let's look at the data first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83227de2",
      "metadata": {
        "scrolled": false,
        "id": "83227de2"
      },
      "outputs": [],
      "source": [
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dfd64b8",
      "metadata": {
        "id": "7dfd64b8"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63bd9f0b",
      "metadata": {
        "id": "63bd9f0b"
      },
      "outputs": [],
      "source": [
        "train.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc88c7f",
      "metadata": {
        "scrolled": true,
        "id": "4dc88c7f"
      },
      "outputs": [],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "396fa03f",
      "metadata": {
        "id": "396fa03f"
      },
      "outputs": [],
      "source": [
        "print(train.isnull().sum())\n",
        "print(test.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a855e8",
      "metadata": {
        "id": "e5a855e8"
      },
      "source": [
        "## EDA:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393726a9",
      "metadata": {
        "id": "393726a9"
      },
      "source": [
        "### 1. Exploring sentiment class distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59fe1408",
      "metadata": {
        "scrolled": false,
        "id": "59fe1408"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x = 'sentiment', data = train)\n",
        "plt.title('Sentiment Class Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cef9138",
      "metadata": {
        "id": "0cef9138"
      },
      "source": [
        "### 2. Text Analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3d98f5",
      "metadata": {
        "id": "4f3d98f5"
      },
      "outputs": [],
      "source": [
        "# Calculate tweet lengths (number of characters and words)\n",
        "train['char_count'] = train['text'].apply(len)\n",
        "train['word_count'] = train['text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Visualize tweet length distributions\n",
        "sns.histplot(data=train, x='char_count', hue='sentiment', bins=30, kde=True)\n",
        "plt.title('Tweet Length Distribution by Sentiment')\n",
        "plt.show()\n",
        "\n",
        "sns.histplot(data=train, x='word_count', hue='sentiment', bins=30, kde=True)\n",
        "plt.title('Word Count Distribution by Sentiment')\n",
        "plt.show()\n",
        "\n",
        "# Create Word Clouds for each sentiment class\n",
        "def plot_wordcloud(sentiment):\n",
        "    text = ' '.join(train[train['sentiment'] == sentiment]['text'])\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', width=800, height=400).generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Word Cloud for {sentiment} Tweets')\n",
        "    plt.show()\n",
        "\n",
        "plot_wordcloud('positive')\n",
        "plot_wordcloud('negative')\n",
        "plot_wordcloud('neutral')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab003942",
      "metadata": {
        "id": "ab003942"
      },
      "source": [
        "### 3. Word Frequency Analysis:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51a9d91e",
      "metadata": {
        "id": "51a9d91e"
      },
      "source": [
        "##### You can download the \"stopwords\" data directly using the code:\n",
        "\"import nltk\n",
        "nltk.download('stopwords')\"\n",
        "\n",
        "However, I was facing some internet issues, hence I manually downloaded the file and defined the function to read the same."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "7-zyA9KS7PEd"
      },
      "id": "7-zyA9KS7PEd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9de306",
      "metadata": {
        "id": "8a9de306"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def word_frequency_analysis(data):\n",
        "\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    # Rest of the function remains the same\n",
        "    exclude = set(string.punctuation)\n",
        "    words = []\n",
        "    for tweet in data['text']:\n",
        "        tweet_words = tweet.lower().split()\n",
        "        for word in tweet_words:\n",
        "            if word not in stop_words and word not in exclude:\n",
        "                words.append(word)\n",
        "\n",
        "    word_counts = Counter(words)\n",
        "    most_common = word_counts.most_common(20)\n",
        "\n",
        "    print(most_common)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(range(len(most_common)), [x[1] for x in most_common], color='blue')\n",
        "    plt.xticks(range(len(most_common)), [x[0] for x in most_common])\n",
        "    plt.title('Word Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have the 'train' DataFrame\n",
        "word_frequency_analysis(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1853a81b",
      "metadata": {
        "id": "1853a81b"
      },
      "source": [
        "#### Some insights: Before we start analyzing the data, let's take a look at some things that we already know about it. This will help us to gain more insights from the data.\n",
        "\n",
        "- We know that selected_text is a subset of text. This means that selected_text is a part of text, but it is not the entire text. <br>\n",
        "</br>\n",
        "- We know that selected_text does not jump between two sentences. This means that selected_text is a continuous segment of text. For example, if the text is \"Spent the entire morning in a meeting with a vendor, and my boss was not happy with them. Lots of fun. I had other plans for my morning\", then selected_text could be \"my boss was not happy with them. Lots of fun\", but it could not be \"Morning, vendor and my boss\". <br>\n",
        "</br>\n",
        "- We know that neutral tweets have a jaccard similarity of 97 percent between text and selected_text. This means that for neutral tweets, the selected text is very similar to the entire text. <br>\n",
        "</br>\n",
        "- We know that there are rows where selected_text starts from between the words. This means that there are some tweets where the selected text does not start at the beginning of a word. This can make it difficult to analyze the data, because it is not clear what the selected text is referring to. </br>\n",
        "- We do not know whether the output of the test set contains these discrepancies. This means that we do not know if the test set will have the same problems with the selected text as the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff55ae2c",
      "metadata": {
        "id": "ff55ae2c"
      },
      "source": [
        "### Let's look at some meta features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eaca200",
      "metadata": {
        "id": "1eaca200"
      },
      "outputs": [],
      "source": [
        "def jaccard_distance(row):\n",
        "    # Convert the text and selected_text to sets of words\n",
        "    text_words = set(row['text'].lower().split())\n",
        "    selected_text_words = set(row['selected_text'].lower().split())\n",
        "\n",
        "    # Calculate the intersection and union of the sets\n",
        "    intersection = len(text_words.intersection(selected_text_words))\n",
        "    union = len(text_words.union(selected_text_words))\n",
        "\n",
        "    # Calculate the Jaccard similarity and distance\n",
        "    jaccard_similarity = intersection / union\n",
        "    jaccard_distance = 1 - jaccard_similarity\n",
        "\n",
        "    return jaccard_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1709eb8",
      "metadata": {
        "id": "a1709eb8"
      },
      "outputs": [],
      "source": [
        "train['jaccard_distance'] = train.apply(jaccard_distance, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c966920",
      "metadata": {
        "id": "9c966920"
      },
      "outputs": [],
      "source": [
        "train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\n",
        "train['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\n",
        "train['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d15d32ea",
      "metadata": {
        "id": "d15d32ea"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f813d002",
      "metadata": {
        "id": "f813d002"
      },
      "outputs": [],
      "source": [
        "def jaccard_score(row):\n",
        "    # Convert the text and selected_text to sets of words\n",
        "    text_words = set(row['text'].lower().split())\n",
        "    selected_text_words = set(row['selected_text'].lower().split())\n",
        "\n",
        "    # Calculate the intersection and union of the sets\n",
        "    intersection = len(text_words.intersection(selected_text_words))\n",
        "    union = len(text_words.union(selected_text_words))\n",
        "\n",
        "    # Calculate the Jaccard score\n",
        "    if union != 0:\n",
        "        jaccard_score = intersection / union\n",
        "    else:\n",
        "        jaccard_score = 0.0\n",
        "\n",
        "    return jaccard_score\n",
        "\n",
        "train['jaccard_score'] = train.apply(jaccard_score, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b4bd7c",
      "metadata": {
        "id": "92b4bd7c"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a9e957",
      "metadata": {
        "id": "f9a9e957"
      },
      "outputs": [],
      "source": [
        "# Plot histogram for Number of words in Selected Text\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=train, x='Num_words_ST', bins=20, kde=True)\n",
        "plt.title('Distribution of Number of Words in Selected Text')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Plot histogram for Number of words in main text\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=train, x='Num_word_text', bins=20, kde=True)\n",
        "plt.title('Distribution of Number of Words in Main Text')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Plot histogram for Difference in Number of words text and Selected Text\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=train, x='difference_in_words', bins=20, kde=True)\n",
        "plt.title('Distribution of Difference in Number of Words (Text - Selected Text)')\n",
        "plt.xlabel('Difference in Number of Words')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Plot histogram for Jaccard Score\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=train, x='jaccard_score', bins=20, kde=True)\n",
        "plt.title('Distribution of Jaccard Score')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec9f22e",
      "metadata": {
        "id": "1ec9f22e"
      },
      "source": [
        "#### We can infer the following:\n",
        "\n",
        "* Number of Words in Selected Text: The histogram for the number of words in the selected text shows that a significant portion of selected text contains around 1 to 4 words. This suggests that the majority of selected text segments are concise and often represent short phrases or single words extracted from the main text.\n",
        "</br>\n",
        "</br>\n",
        "* Number of Words in Main Text: The histogram for the number of words in the main text indicates that the main text of tweets generally consists of around 5 to 25 words. This suggests that tweets are generally of moderate length, with the majority containing a few sentences or phrases.\n",
        "</br>\n",
        "</br>\n",
        "* Difference in Number of Words: The histogram for the difference in the number of words between the main text and selected text shows that most selected text segments are shorter than the main text. This is consistent with our previous understanding that the selected text is usually a subset of the main text, capturing a specific segment that represents the sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85e64fc8",
      "metadata": {
        "id": "85e64fc8"
      },
      "outputs": [],
      "source": [
        "k = train[train['Num_word_text']<=2]\n",
        "k.groupby('sentiment').mean()['jaccard_score']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8003debc",
      "metadata": {
        "id": "8003debc"
      },
      "outputs": [],
      "source": [
        "k[k['sentiment']=='positive']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5f353b",
      "metadata": {
        "id": "2b5f353b"
      },
      "source": [
        "### Cleaning text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba0f334a",
      "metadata": {
        "id": "ba0f334a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove text in square brackets\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "\n",
        "    # Remove links\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove words containing numbers\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "    # Make text lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348130ce",
      "metadata": {
        "id": "348130ce"
      },
      "outputs": [],
      "source": [
        "train['text'] = train['text'].apply(lambda x:clean_text(x))\n",
        "train['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa6171b",
      "metadata": {
        "id": "5fa6171b"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd175a2",
      "metadata": {
        "id": "9dd175a2"
      },
      "source": [
        "### Let's now find the most common work in our target feature: \"Selected-Text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a20171",
      "metadata": {
        "id": "49a20171"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply remove_stopwords to the 'selected_text' column in the train DataFrame\n",
        "train['cleaned_selected_text'] = train['selected_text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecae571e",
      "metadata": {
        "id": "ecae571e"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the selected texts into a single string\n",
        "all_selected_texts = ' '.join(train['cleaned_selected_text'])\n",
        "\n",
        "# Split the text into words and count their frequencies\n",
        "word_counts = Counter(all_selected_texts.split())\n",
        "\n",
        "# Get the most common words\n",
        "most_common_words = word_counts.most_common(10)\n",
        "words, counts = zip(*most_common_words)\n",
        "\n",
        "print(most_common_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3aed9f6",
      "metadata": {
        "id": "a3aed9f6"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create a DataFrame for the most common words\n",
        "df_most_common = pd.DataFrame({'words': words, 'counts': counts})\n",
        "\n",
        "# Plot the treemap\n",
        "fig = px.treemap(df_most_common, path=['words'], values='counts')\n",
        "fig.update_layout(title='Top 10 Most Common Words in Cleaned Selected Text (Treemap)')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38de7101",
      "metadata": {
        "id": "38de7101"
      },
      "source": [
        "#### Similarly, let's find the most common words in \"Text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3f9cd5",
      "metadata": {
        "id": "5f3f9cd5"
      },
      "outputs": [],
      "source": [
        "train['cleaned_text'] = train['text'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad04d7a",
      "metadata": {
        "id": "cad04d7a"
      },
      "outputs": [],
      "source": [
        "# Concatenate all the texts into a single string\n",
        "all_texts = ' '.join(train['cleaned_text'])\n",
        "\n",
        "# Split the text into words and count their frequencies\n",
        "word_counts = Counter(all_texts.split())\n",
        "\n",
        "# Get the most common words\n",
        "most_common_words = word_counts.most_common(10)\n",
        "words, counts = zip(*most_common_words)\n",
        "\n",
        "print(most_common_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d568aa",
      "metadata": {
        "id": "23d568aa"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for the most common words\n",
        "df_most_common = pd.DataFrame({'words': words, 'counts': counts})\n",
        "\n",
        "# Plot the treemap\n",
        "fig = px.treemap(df_most_common, path=['words'], values='counts')\n",
        "fig.update_layout(title='Top 10 Most Common Words in Selected Text')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ddbfc6",
      "metadata": {
        "id": "25ddbfc6"
      },
      "source": [
        "### Clearly, the most common words in both \"selected text\" and \"text\" are similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8db2f61",
      "metadata": {
        "id": "d8db2f61"
      },
      "outputs": [],
      "source": [
        "# Filter the data by sentiment categories\n",
        "positive_texts = ' '.join(train[train['sentiment'] == 'positive']['cleaned_selected_text'])\n",
        "negative_texts = ' '.join(train[train['sentiment'] == 'negative']['cleaned_selected_text'])\n",
        "neutral_texts = ' '.join(train[train['sentiment'] == 'neutral']['cleaned_selected_text'])\n",
        "\n",
        "# Split the text into words and count their frequencies for each sentiment\n",
        "positive_word_counts = Counter(positive_texts.split())\n",
        "negative_word_counts = Counter(negative_texts.split())\n",
        "neutral_word_counts = Counter(neutral_texts.split())\n",
        "\n",
        "# Get the most common words for each sentiment\n",
        "most_common_positive = positive_word_counts.most_common(10)\n",
        "most_common_negative = negative_word_counts.most_common(10)\n",
        "most_common_neutral = neutral_word_counts.most_common(10)\n",
        "\n",
        "print(\"Most Common Words in Positive Sentiment:\")\n",
        "print(most_common_positive)\n",
        "\n",
        "print(\"\\nMost Common Words in Negative Sentiment:\")\n",
        "print(most_common_negative)\n",
        "\n",
        "print(\"\\nMost Common Words in Neutral Sentiment:\")\n",
        "print(most_common_neutral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca6e2c3",
      "metadata": {
        "id": "8ca6e2c3"
      },
      "outputs": [],
      "source": [
        "# Define the data for the heatmap\n",
        "data = {\n",
        "    'Word': [word for word, _ in most_common_positive + most_common_negative + most_common_neutral],\n",
        "    'Sentiment': ['Positive'] * len(most_common_positive) + ['Negative'] * len(most_common_negative) + ['Neutral'] * len(most_common_neutral),\n",
        "    'Frequency': [count for _, count in most_common_positive + most_common_negative + most_common_neutral]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "# Convert the 'Frequency' column to integers\n",
        "df['Frequency'] = df['Frequency'].astype(int)\n",
        "\n",
        "# Create the heatmap using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.pivot('Sentiment', 'Word', 'Frequency'), cmap='YlGnBu', annot=True, fmt='.0f', cbar_kws={'label': 'Frequency'})\n",
        "plt.title('Most Common Words Sentiments-Wise')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Sentiment')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38c46290",
      "metadata": {
        "id": "38c46290"
      },
      "outputs": [],
      "source": [
        "# Get the most common positive words\n",
        "positive_words, positive_counts = zip(*most_common_positive)\n",
        "\n",
        "# Plot the most common positive words using a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(positive_words, positive_counts, color='blue')\n",
        "plt.title('Most Common Positive Words')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "# Get the most common negative words\n",
        "negative_words, negative_counts = zip(*most_common_negative)\n",
        "\n",
        "# Plot the most common negative words using a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(negative_words, negative_counts, color='red')\n",
        "plt.title('Most Common Negative Words')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "# Get the most common neutral words\n",
        "neutral_words, neutral_counts = zip(*most_common_neutral)\n",
        "\n",
        "# Plot the most common neutral words using a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(neutral_words, neutral_counts, color='green')\n",
        "plt.title('Most Common Neutral Words')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b1b3f6",
      "metadata": {
        "id": "32b1b3f6"
      },
      "source": [
        "### Let's Look at Unique Words in each Segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c1ea2a",
      "metadata": {
        "id": "f8c1ea2a"
      },
      "outputs": [],
      "source": [
        "# Find unique words in each segment\n",
        "unique_positive_words = set(positive_word_counts.keys())\n",
        "unique_negative_words = set(negative_word_counts.keys())\n",
        "unique_neutral_words = set(neutral_word_counts.keys())\n",
        "\n",
        "# Find words unique to each sentiment\n",
        "words_unique_to_positive = unique_positive_words - unique_negative_words - unique_neutral_words\n",
        "words_unique_to_negative = unique_negative_words - unique_positive_words - unique_neutral_words\n",
        "words_unique_to_neutral = unique_neutral_words - unique_positive_words - unique_negative_words\n",
        "\n",
        "print(\"Words Unique to Positive Sentiment:\")\n",
        "print(words_unique_to_positive)\n",
        "\n",
        "print(\"\\nWords Unique to Negative Sentiment:\")\n",
        "print(words_unique_to_negative)\n",
        "\n",
        "print(\"\\nWords Unique to Neutral Sentiment:\")\n",
        "print(words_unique_to_neutral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9176eeb",
      "metadata": {
        "id": "d9176eeb"
      },
      "outputs": [],
      "source": [
        "# Get the most common unique words in positive tweets\n",
        "most_common_unique_positive = Counter(words_unique_to_positive).most_common(20)\n",
        "print(\"The Top 20 Unique Words in Positive Tweets are:\")\n",
        "print(most_common_unique_positive)\n",
        "\n",
        "# Get the most common unique words in negative tweets\n",
        "most_common_unique_negative = Counter(words_unique_to_negative).most_common(20)\n",
        "print(\"The Top 20 Unique Words in Negative Tweets are:\")\n",
        "print(most_common_unique_negative)\n",
        "\n",
        "# Get the most common unique words in positive tweets\n",
        "most_common_unique_neutral = Counter(words_unique_to_neutral).most_common(20)\n",
        "print(\"The Top 20 Unique Words in Neutral Tweets are:\")\n",
        "print(most_common_unique_neutral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abb8a0e",
      "metadata": {
        "id": "5abb8a0e"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for each sentiment category\n",
        "df_positive = pd.DataFrame(most_common_unique_positive, columns=['word', 'count'])\n",
        "df_negative = pd.DataFrame(most_common_unique_negative, columns=['word', 'count'])\n",
        "df_neutral = pd.DataFrame(most_common_unique_neutral, columns=['word', 'count'])\n",
        "\n",
        "# Plot treemaps for each sentiment category\n",
        "fig_positive = px.treemap(df_positive, path=['word'], values='count', title='Top 20 Unique Words in Positive Tweets')\n",
        "fig_positive.show()\n",
        "\n",
        "fig_negative = px.treemap(df_negative, path=['word'], values='count', title='Top 20 Unique Words in Negative Tweets')\n",
        "fig_negative.show()\n",
        "\n",
        "fig_neutral = px.treemap(df_neutral, path=['word'], values='count', title='Top 20 Unique Words in Neutral Tweets')\n",
        "fig_neutral.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786bb1bd",
      "metadata": {
        "id": "786bb1bd"
      },
      "source": [
        "## WordClouds:\n",
        "\n",
        "#### We will be building wordclouds in the following order:\n",
        "\n",
        "#### - WordCloud of Neutral Tweets\n",
        "#### - WordCloud of Positive Tweets\n",
        "#### - WordCloud of Negative Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4c7190",
      "metadata": {
        "id": "2a4c7190"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Function to create WordCloud with custom parameters\n",
        "def create_wordcloud(text, title):\n",
        "    wordcloud = WordCloud(width=1000, height=600, background_color='white',\n",
        "                          colormap='viridis', max_words=100, contour_width=3, contour_color='steelblue',\n",
        "                          random_state=42).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title, fontsize=20)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Join the text of each sentiment category into a single string\n",
        "positive_texts = ' '.join(train[train['sentiment'] == 'positive']['cleaned_selected_text'])\n",
        "negative_texts = ' '.join(train[train['sentiment'] == 'negative']['cleaned_selected_text'])\n",
        "neutral_texts = ' '.join(train[train['sentiment'] == 'neutral']['cleaned_selected_text'])\n",
        "\n",
        "# Create WordCloud for each sentiment category\n",
        "create_wordcloud(positive_texts, 'Word Cloud for Positive Tweets')\n",
        "create_wordcloud(negative_texts, 'Word Cloud for Negative Tweets')\n",
        "create_wordcloud(neutral_texts, 'Word Cloud for Neutral Tweets')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5ea61e",
      "metadata": {
        "id": "ae5ea61e"
      },
      "source": [
        "## Finally time for \"Modelling\":"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55497558",
      "metadata": {
        "id": "55497558"
      },
      "source": [
        "#### To build the sentiment extraction model, we will be using the spaCy library for named entity recognition (NER) and sequence-to-sequence modeling. The task involves predicting the start and end positions of the selected text in the original tweet. We'll use the spaCy library, which is a powerful NLP library that provides pre-trained models and allows us to train our own models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9276bcad",
      "metadata": {
        "id": "9276bcad"
      },
      "outputs": [],
      "source": [
        "train['Num_words_text'] = train['text'].apply(lambda x: len(str(x).split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c63137c0",
      "metadata": {
        "id": "c63137c0"
      },
      "outputs": [],
      "source": [
        "train = train[train['Num_words_text']>3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(output_dir, nlp, new_model_name):\n",
        "    ''' This Function Saves model to\n",
        "    given output directory'''\n",
        "\n",
        "    output_dir = f'../working/{output_dir}'\n",
        "    if output_dir is not None:\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        nlp.meta[\"name\"] = new_model_name\n",
        "        nlp.to_disk(output_dir)\n",
        "        print(\"Saved model to\", output_dir)\n",
        "\n",
        "output_dir = '/content'"
      ],
      "metadata": {
        "id": "Q5aj7k9RBD3f"
      },
      "id": "Q5aj7k9RBD3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from spacy.util import minibatch\n",
        "from spacy.training.example import Example\n",
        "\n",
        "def train_ner(train_data, output_dir, n_iter=20, model=None):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    \"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(output_dir)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe('ner', last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # sizes = compounding(1.0, 4.0, 1.001)\n",
        "        # batch up the examples using spaCy's minibatch\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        else:\n",
        "            nlp.resume_training()\n",
        "\n",
        "\n",
        "        for itn in tqdm(range(n_iter)):\n",
        "          random.shuffle(train_data)\n",
        "          losses = {}\n",
        "          for batch in minibatch(train_data, size=compounding(4.0, 500.0, 1.001)):\n",
        "                texts, annotations = zip(*batch)\n",
        "                examples = []\n",
        "                for i in range(len(texts)):\n",
        "                      doc = nlp.make_doc(texts[i])\n",
        "                      example = Example.from_dict(doc, annotations[i])\n",
        "                      examples.append(example)\n",
        "                      nlp.update(examples, drop=0.5, losses=losses)\n",
        "                print(\"Losses\", losses)\n",
        "    save_model(output_dir, nlp, 'st_ner')"
      ],
      "metadata": {
        "id": "bl4UBT9PBD6b"
      },
      "id": "bl4UBT9PBD6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_out_path(sentiment):\n",
        "    '''\n",
        "    Returns Model output path\n",
        "    '''\n",
        "    model_out_path = None\n",
        "    if sentiment == 'positive':\n",
        "        model_out_path = '/content/models/model_pos'\n",
        "    elif sentiment == 'negative':\n",
        "        model_out_path = '/content/models/model_neg'\n",
        "    return model_out_path"
      ],
      "metadata": {
        "id": "Nw1R82JYBD92"
      },
      "id": "Nw1R82JYBD92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_data(sentiment):\n",
        "    '''\n",
        "    Returns Trainong data in the format needed to train spacy NER\n",
        "    '''\n",
        "    train_data = []\n",
        "    for index, row in train.iterrows():\n",
        "        if row.sentiment == sentiment:\n",
        "            selected_text = row.selected_text\n",
        "            text = row.text\n",
        "            start = text.find(selected_text)\n",
        "            end = start + len(selected_text)\n",
        "            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n",
        "    return train_data"
      ],
      "metadata": {
        "id": "rLSw5u53BEAv"
      },
      "id": "rLSw5u53BEAv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training models for Postive and Negative Tweets"
      ],
      "metadata": {
        "id": "ZaxHbDDCKvGR"
      },
      "id": "ZaxHbDDCKvGR"
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = 'positive'\n",
        "\n",
        "train_data = get_training_data(sentiment)\n",
        "model_path = get_model_out_path(sentiment)\n",
        "# For DEmo Purposes I have taken 3 iterations you can train the model as you want\n",
        "train_ner(train_data, model_path, n_iter=3, model=None)"
      ],
      "metadata": {
        "id": "aYQz6ykaK6Ue"
      },
      "id": "aYQz6ykaK6Ue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = 'negative'\n",
        "\n",
        "train_data = get_training_data(sentiment)\n",
        "model_path = get_model_out_path(sentiment)\n",
        "\n",
        "train_ner(train_data, model_path, n_iter=3, model=None)"
      ],
      "metadata": {
        "id": "Wj380sqsRkJP"
      },
      "id": "Wj380sqsRkJP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting with the trained Model"
      ],
      "metadata": {
        "id": "GJvPC0XVKslL"
      },
      "id": "GJvPC0XVKslL"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_entities(text, model):\n",
        "    doc = model(text)\n",
        "    ent_array = []\n",
        "    for ent in doc.ents:\n",
        "        start = text.find(ent.text)\n",
        "        end = start + len(ent.text)\n",
        "        new_int = [start, end, ent.label_]\n",
        "        if new_int not in ent_array:\n",
        "            ent_array.append([start, end, ent.label_])\n",
        "    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n",
        "    return selected_text"
      ],
      "metadata": {
        "id": "FnUfQOfPBEMO"
      },
      "id": "FnUfQOfPBEMO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d rohitsingh9990/tse-spacy-model"
      ],
      "metadata": {
        "id": "198D5oHQmJrQ"
      },
      "id": "198D5oHQmJrQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tse-spacy-model.zip"
      ],
      "metadata": {
        "id": "LRaSN1YimSaf"
      },
      "id": "LRaSN1YimSaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_texts = []\n",
        "\n",
        "# Load the models from the saved paths\n",
        "model_pos = spacy.load('/working/models/model_neg/ner/model')\n",
        "model_neg = spacy.load('/working/models/model_pos/ner/model')\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    text = row.text\n",
        "    output_str = \"\"\n",
        "    if row.sentiment == 'neutral' or len(text.split()) <= 2:\n",
        "        selected_texts.append(text)\n",
        "    elif row.sentiment == 'positive':\n",
        "        selected_texts.append(predict_entities(text, model_pos))\n",
        "    else:\n",
        "        selected_texts.append(predict_entities(text, model_neg))\n",
        "\n",
        "test['selected_text'] = selected_texts"
      ],
      "metadata": {
        "id": "1sgVA7eHBERK"
      },
      "id": "1sgVA7eHBERK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mPyPMR_9BEcz"
      },
      "id": "mPyPMR_9BEcz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CcgCTpfvBEem"
      },
      "id": "CcgCTpfvBEem",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tVnKpEozBEjq"
      },
      "id": "tVnKpEozBEjq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1624e1fc",
      "metadata": {
        "id": "1624e1fc"
      },
      "source": [
        "Why Twitter Sentiment Extraction Matters:\n",
        "\n",
        "Business Insights: For companies, understanding the sentiments of customers on Twitter can offer invaluable insights into brand perception, customer satisfaction, and areas of improvement.\n",
        "\n",
        "Social Media Marketing: Marketers can utilize sentiment analysis to gauge the success of their marketing campaigns and adjust strategies accordingly.\n",
        "\n",
        "Market Research: Sentiment analysis can aid market researchers in understanding consumer opinions, preferences, and trends.\n",
        "\n",
        "Public Opinion Analysis: Policymakers and researchers can analyze public opinion on social issues, events, or political matters.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "The \"Twitter Sentiment Extraction\" project combines sentiment analysis and text extraction techniques to gain in-depth insights from Twitter data. By building an accurate sentiment analysis model and extracting key words or phrases, this project aims to unlock valuable information hidden within the vast landscape of tweets. The outcomes can empower businesses, marketers, researchers, and decision-makers with actionable insights to improve their strategies, products, and services based on the sentiments of the Twitterverse."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f3eeecc",
      "metadata": {
        "id": "8f3eeecc"
      },
      "source": [
        "Acknowledgements\n",
        "1. https://www.kaggle.com/aashita/word-clouds-of-various-shapes --> WORDCLOUDS FUNCTION\n",
        "2. https://www.kaggle.com/rohitsingh9990/ner-training-using-spacy-0-628-lb --> For understanding how to train spacy NER on custom inputs\n",
        "3. https://www.kaggle.com/code/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model/notebook"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}