{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08e788d",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "This dataset contains transactions of September 2013, by citizens of the EU region. \n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "Due to security issues, there are no information about original features and more background info about the data (I believe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681e027",
   "metadata": {},
   "source": [
    "## Importing the data directly from Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb282bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = '.\\creditcardfraud'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367af4a8",
   "metadata": {},
   "source": [
    "## Loading the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# TRAIN/VALIDATION/TEST SPLIT\n",
    "# VALIDATION\n",
    "VALID_SIZE = 0.20  # simple validation using train_test_split\n",
    "TEST_SIZE = 0.20  # test size using_train_test_split\n",
    "\n",
    "# CROSS-VALIDATION\n",
    "NUMBER_KFOLDS = 5  # number of KFolds for cross-validation\n",
    "\n",
    "RANDOM_STATE = 2018\n",
    "\n",
    "MAX_ROUNDS = 1000  # lgb iterations\n",
    "EARLY_STOP = 50  # lgb early stop\n",
    "OPT_ROUNDS = 1000  # To be adjusted based on best validation rounds\n",
    "VERBOSE_EVAL = 50  # Print out metric result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30b1b7",
   "metadata": {},
   "source": [
    "### Reading and Checking the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf6480",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"C:/Users/Souptik/creditcardfraud/creditcard.csv\")\n",
    "print(\"Credit Card Fraud Detection data -  rows:\",\n",
    "      data_df.shape[0],\" columns:\", data_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ba94c",
   "metadata": {},
   "source": [
    "#### Checking for missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ba2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (data_df.isnull().sum() / data_df.isnull().count() * 100).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()\n",
    "print(\"Missing Data:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f0906",
   "metadata": {},
   "source": [
    "##### Clearly, there are no missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25037a29",
   "metadata": {},
   "source": [
    "### Data Unbalance:\n",
    "##### Let's check data unbalance with respect to \"Target\" value, which is \"Class\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8495369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to hold the class counts\n",
    "temp = data_df[\"Class\"].value_counts()\n",
    "df = pd.DataFrame({'Class': temp.index, 'values': temp.values})\n",
    "\n",
    "# Create the bar chart\n",
    "trace = go.Bar(\n",
    "    x=df['Class'],\n",
    "    y=df['values'],\n",
    "    name=\"Credit Card Fraud Class - data unbalance (Not fraud=0, Fraud=1)\",\n",
    "    marker=dict(color=\"Red\"),\n",
    "    text=df['values']\n",
    ")\n",
    "data = [trace]\n",
    "layout = dict(\n",
    "    title='Credit Card Fraud Class - data unbalance (Not fraud=0, Fraud=1)',\n",
    "    xaxis=dict(title='Class', showticklabels=True),\n",
    "    yaxis=dict(title='Number of transactions'),\n",
    "    hovermode='closest',\n",
    "    width=600\n",
    ")\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6abfa",
   "metadata": {},
   "source": [
    "##### Only 492 (or 0.172%) of transactions are fraudulent, which indicates that the data is highly unbalanced with respect to the target variable \"Class\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db46102",
   "metadata": {},
   "source": [
    "## EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd1d0d",
   "metadata": {},
   "source": [
    "#### 1. Histogram for \"Transaction in Time\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd72bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.histogram(data_df, x='Time', color='Class', nbins=50,\n",
    "                   labels={'Time': 'Time [s]', 'Class': 'Class'},\n",
    "                   title='Credit Card Transactions Time Density Plot',\n",
    "                   barmode='overlay', histnorm='probability density')\n",
    "\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae413525",
   "metadata": {},
   "source": [
    "##### We can infer the following:\n",
    "\n",
    "- Transaction Time Distribution: The plot shows the distribution of transaction times in seconds (Time [s]) for both fraud and non-fraud transactions. The x-axis represents the transaction time, and the y-axis shows the probability density of the transactions.\n",
    "\n",
    "- Peak Times: The plot allows us to identify the peak times when most transactions occur. For non-fraud transactions (class 0), there are one or more peaks where a large number of legitimate transactions occur. For fraud transactions (class 1), there may be different peak times or patterns compared to non-fraud transactions, which could indicate potential anomalies.\n",
    "\n",
    "- Transaction Time Differences: We can observe if there are any notable differences in transaction time distributions between fraud and non-fraud transactions. Differences in peak times or shapes of the distributions may suggest potential patterns or anomalies in the fraud transactions.\n",
    "\n",
    "- Overlapping Areas: The overlapping areas in the plot represent regions where both fraud and non-fraud transactions occur similarly in terms of transaction times. This overlap can be important in distinguishing between fraud and non-fraud transactions, as some fraudulent activities might be similar to legitimate transactions in terms of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c1c25",
   "metadata": {},
   "source": [
    "#### Let's aggregate the data by hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Hour'] = data_df['Time'].apply(lambda x: np.floor(x/3600))\n",
    "\n",
    "tmp = data_df.groupby(['Hour', 'Class'])['Amount'].aggregate(['min', 'max', 'count', 'sum', 'mean', 'median', 'var']).reset_index()\n",
    "df = pd.DataFrame(tmp)\n",
    "\n",
    "df.columns = ['Hour', 'Class', 'Min', 'Max', 'Transactions', 'Sum',\n",
    "             'Mean', 'Median', 'Var']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Sum\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Sum\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Total Amount\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Transactions\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Transactions\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Total Number of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Average Amount of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c509e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Max\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Max\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Maximum Amount of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Median\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Median\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Median Amount of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483664f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Min\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Min\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Minimum Amount of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc9c6c",
   "metadata": {},
   "source": [
    "#### Transactions amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "s = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\n",
    "s = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data_df[['Amount','Class']].copy()\n",
    "class_0 = tmp.loc[tmp['Class'] == 0]['Amount']\n",
    "class_1 = tmp.loc[tmp['Class'] == 1]['Amount']\n",
    "class_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163e079",
   "metadata": {},
   "source": [
    "The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd9142",
   "metadata": {},
   "source": [
    "##### Let's plot the fraudulent transactions (amount) against time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudulent_transactions = data_df[data_df['Class'] == 1]\n",
    "\n",
    "# Create a scatter plot for fraudulent transactions (amount) against time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(fraudulent_transactions['Time'], fraudulent_transactions['Amount'], color='red', alpha=0.7)\n",
    "plt.title('Fraudulent Transactions (Amount) vs Time')\n",
    "plt.xlabel('Time (seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5815ba",
   "metadata": {},
   "source": [
    "#### Moving on the Features engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af3b45",
   "metadata": {},
   "source": [
    "##### Features correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01fbf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = data_df.corr()\n",
    "\n",
    "# Perform hierarchical clustering to reorder the rows and columns\n",
    "g = sns.clustermap(corr_matrix, cmap='coolwarm', center=0, annot=True, fmt=\".2f\",\n",
    "                   linewidths=.5, cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title('Credit Card Transactions Features Correlation Heatmap')\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.setp(g.ax_heatmap.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ed91e",
   "metadata": {},
   "source": [
    "- There are no notable correlations between features V1-V28. This means that these features are relatively independent of each other and don't show strong linear relationships.\n",
    "\n",
    "- There are certain correlations between some of these features and Time:\n",
    "\n",
    "    * V3 shows an inverse correlation with Time, indicating that as Time increases, V3 tends to decrease (or vice versa). This suggests that there might be some time-dependent patterns in the data related to V3.\n",
    "- There are certain correlations between some of these features and Amount:\n",
    "\n",
    "    * V7 and V20 show a direct correlation with Amount, indicating that as Amount increases, V7 and V20 tend to increase as well. This suggests that there might be some relationship between the transaction amount and these features.\n",
    "    * V1 and V5 show an inverse correlation with Amount, suggesting that as Amount increases, V1 and V5 tend to decrease (or vice versa). This could indicate some patterns related to the transaction amount and these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfca52",
   "metadata": {},
   "source": [
    "#### Let's plot the correlated and inverse correlated values on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c065591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame with the correlated and inverse correlated values\n",
    "correlated_df = data_df[['Amount', 'V7', 'V20']].copy()\n",
    "inverse_correlated_df = data_df[['Amount', 'V1', 'V5']].copy()\n",
    "\n",
    "# Creating subplots for correlated and inverse correlated plots\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18, 10))\n",
    "\n",
    "# Plotting the correlated values\n",
    "sns.scatterplot(x='Amount', y='V7', data=correlated_df, ax=ax1, color='blue', label='V7 (Correlated)')\n",
    "sns.scatterplot(x='Amount', y='V20', data=correlated_df, ax=ax1, color='green', label='V20 (Correlated)')\n",
    "ax1.set_title('Correlated Features vs. Amount')\n",
    "ax1.legend()\n",
    "\n",
    "# Plotting the inverse correlated values\n",
    "sns.scatterplot(x='Amount', y='V1', data=inverse_correlated_df, ax=ax2, color='red', label='V1 (Inverse Correlated)')\n",
    "sns.scatterplot(x='Amount', y='V5', data=inverse_correlated_df, ax=ax2, color='orange', label='V5 (Inverse Correlated)')\n",
    "ax2.set_title('Inverse Correlated Features vs. Amount')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f653e6",
   "metadata": {},
   "source": [
    "From the output, we can understand the following:\n",
    "\n",
    "1. Correlated Features (V7 and V20) vs. Amount:\n",
    "\n",
    "  * V7 (Correlated): As the transaction Amount increases, the values of V7 tend to increase as well. There is a positive correlation between V7 and the transaction Amount.\n",
    "  * V20 (Correlated): Similarly, as the transaction Amount increases, the values of V20 also tend to increase. There is a positive correlation between V20 and the transaction Amount.\n",
    "\n",
    "2. Inverse Correlated Features (V1 and V5) vs. Amount:\n",
    "\n",
    "  * V1 (Inverse Correlated): As the transaction Amount increases, the values of V1 tend to decrease. There is a negative (inverse) correlation between V1 and the transaction Amount.\n",
    "  * V5 (Inverse Correlated): Similarly, as the transaction Amount increases, the values of V5 tend to decrease. There is a negative (inverse) correlation between V5 and the transaction Amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2a620",
   "metadata": {},
   "source": [
    "#### Features density plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['V1', 'V5', 'V7', 'V20']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(selected_features), ncols=1, figsize=(8, 12), sharex=True)\n",
    "\n",
    "# Plotting the density plot for each feature\n",
    "for i, feature in enumerate(selected_features):\n",
    "    sns.kdeplot(data_df[data_df['Class'] == 0][feature], label='Not Fraud', ax=axes[i], color='blue', linewidth=2)\n",
    "    sns.kdeplot(data_df[data_df['Class'] == 1][feature], label='Fraud', ax=axes[i], color='red', linewidth=2)\n",
    "\n",
    "    axes[i].set_title(f'{feature} Density Plot', fontsize=14)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6ca25",
   "metadata": {},
   "source": [
    "#### What if I had to select all features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9998c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = data_df.drop('Class', axis=1).columns.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(selected_features), ncols=1, figsize=(8, 4 * len(selected_features)), sharex=True)\n",
    "\n",
    "for i, feature in enumerate(selected_features):\n",
    "    sns.kdeplot(data_df[data_df['Class'] == 0][feature], label='Not Fraud', ax=axes[i], color='blue', linewidth=2)\n",
    "    sns.kdeplot(data_df[data_df['Class'] == 1][feature], label='Fraud', ax=axes[i], color='red', linewidth=2)\n",
    "\n",
    "    axes[i].set_title(f'{feature} Density Plot', fontsize=14)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be550aa0",
   "metadata": {},
   "source": [
    "#### Summary of my observations:\n",
    "\n",
    "- Good Separation: Features V4 and V11 have clearly separated distributions for Class values 0 and 1, indicating they could be strong indicators for distinguishing between legitimate and fraudulent transactions.\n",
    "\n",
    "- Partial Separation: Features V12, V14, and V18 show partial separation between the two classes, suggesting they still provide some discriminatory power, but there is some overlap in their distributions.\n",
    "\n",
    "- Distinct Profiles: Features V1, V2, V3, and V10 have distinct profiles for the two values of Class, indicating they may also be informative in distinguishing between the classes.\n",
    "\n",
    "- Similar Profiles: Features V25, V26, and V28 have similar profiles for the two values of Class, meaning their distributions are not very informative in differentiating between legitimate and fraudulent transactions.\n",
    "\n",
    "- Centered Around 0: For most features (except Time and Amount), the distributions for Class = 0 (legitimate transactions) are centered around 0, with some having a long tail on one side. This suggests that in general, legitimate transactions tend to have values closer to 0 for these features.\n",
    "\n",
    "- Skewed Distribution: For Class = 1 (fraudulent transactions), the distributions are skewed, indicating that certain features may have extreme values for fraudulent cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f76a38",
   "metadata": {},
   "source": [
    "### Building the models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cfe03",
   "metadata": {},
   "source": [
    "##### Defining the predictors and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Class'\n",
    "predictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n",
    "       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n",
    "       'Amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819f1ea4",
   "metadata": {},
   "source": [
    "##### Splitting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a24c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\n",
    "train_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc15d2",
   "metadata": {},
   "source": [
    "### 1st model: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1897850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=4, \n",
    "                             random_state=42,\n",
    "                             criterion='gini',\n",
    "                             n_estimators=100,\n",
    "                             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(train_df[predictors], train_df[target].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(valid_df[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca214284",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': rf.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb97ef",
   "metadata": {},
   "source": [
    "##### The most important features are: V17, V14, V12, V10, V16, V11 and V9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c47e2b",
   "metadata": {},
   "source": [
    "#### Let's plot the confusion matrix for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(valid_df[target].values, preds)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Not Fraud\", \"Fraud\"], yticklabels=[\"Not Fraud\", \"Fraud\"])\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90640567",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(valid_df[target].values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea8bc9",
   "metadata": {},
   "source": [
    "#### The ROC-AUC score obtained with RandomForrestClassifier is 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9836563",
   "metadata": {},
   "source": [
    "### 2nd Model: LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1deb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# Define hyperparameters for LGBM model\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 7,\n",
    "    'max_depth': 4,\n",
    "    'min_child_samples': 100,\n",
    "    'max_bin': 100,\n",
    "    'subsample': 0.9,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 0,\n",
    "    'min_split_gain': 0,\n",
    "    'nthread': 8,\n",
    "    'verbose': 0,\n",
    "    'scale_pos_weight': 150\n",
    "}\n",
    "\n",
    "# Create the LGBM dataset\n",
    "dtrain = lgb.Dataset(train_df[predictors].values,\n",
    "                     label=train_df[target].values,\n",
    "                     feature_name=predictors)\n",
    "\n",
    "dvalid = lgb.Dataset(valid_df[predictors].values,\n",
    "                     label=valid_df[target].values,\n",
    "                     feature_name=predictors)\n",
    "\n",
    "# Train the LGBM model\n",
    "model = lgb.train(params,\n",
    "                  dtrain,\n",
    "                  valid_sets=[dtrain, dvalid],\n",
    "                  valid_names=['train', 'valid'],\n",
    "                  num_boost_round=MAX_ROUNDS)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "preds = model.predict(valid_df[predictors])\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
    "preds_binary = (preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model using a confusion matrix\n",
    "cm = confusion_matrix(valid_df[target].values, preds_binary)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Not Fraud\", \"Fraud\"], yticklabels=[\"Not Fraud\", \"Fraud\"])\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.title(\"Confusion Matrix (LGBM)\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the ROC AUC score for the LGBM model\n",
    "roc_auc = roc_auc_score(valid_df[target].values, preds)\n",
    "print(\"ROC AUC Score (LGBM):\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0646c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\n",
    "lgb.plot_importance(model, height=0.8, title=\"Features importance (LightGBM)\", ax=ax,color=\"red\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb0423",
   "metadata": {},
   "source": [
    "### 3rd Model: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Define hyperparameters for XGB model\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 0,\n",
    "    'scale_pos_weight': 150\n",
    "}\n",
    "\n",
    "# Create the XGB dataset\n",
    "dtrain = xgb.DMatrix(train_df[predictors].values, label=train_df[target].values)\n",
    "dvalid = xgb.DMatrix(valid_df[predictors].values, label=valid_df[target].values)\n",
    "\n",
    "# Train the XGB model\n",
    "model = xgb.train(params,\n",
    "                  dtrain,\n",
    "                  num_boost_round=MAX_ROUNDS,\n",
    "                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                  early_stopping_rounds=2*EARLY_STOP,\n",
    "                  verbose_eval=VERBOSE_EVAL)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "preds = model.predict(dvalid)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
    "preds_binary = (preds >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model using a confusion matrix\n",
    "cm = confusion_matrix(valid_df[target].values, preds_binary)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"Not Fraud\", \"Fraud\"], yticklabels=[\"Not Fraud\", \"Fraud\"])\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.title(\"Confusion Matrix (XGB)\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the ROC AUC score for the XGB model\n",
    "roc_auc = roc_auc_score(valid_df[target].values, preds)\n",
    "print(\"ROC AUC Score (XGB):\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\n",
    "xgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "\n",
    "kf = KFold(n_splits=NUMBER_KFOLDS, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(train_df.shape[0])\n",
    "test_preds = np.zeros(test_df.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "n_fold = 0\n",
    "\n",
    "for train_idx, valid_idx in kf.split(train_df):\n",
    "    train_x, train_y = train_df[predictors].iloc[train_idx], train_df[target].iloc[train_idx]\n",
    "    valid_x, valid_y = train_df[predictors].iloc[valid_idx], train_df[target].iloc[valid_idx]\n",
    "\n",
    "    # XGBoost model initialization\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "              eval_metric='auc', early_stopping_rounds=2*EARLY_STOP, verbose=VERBOSE_EVAL)\n",
    "\n",
    "    oof_preds[valid_idx] = model.predict_proba(valid_x)[:, 1]\n",
    "    test_preds += model.predict_proba(test_df[predictors])[:, 1] / kf.n_splits\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = predictors\n",
    "    fold_importance_df[\"importance\"] = model.feature_importances_\n",
    "    fold_importance_df[\"fold\"] = n_fold + 1\n",
    "\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "    del model, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()\n",
    "    n_fold = n_fold + 1\n",
    "\n",
    "train_auc_score = roc_auc_score(train_df[target], oof_preds)\n",
    "print('Full AUC score %.6f' % train_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c96c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c64915",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca179b",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb37446",
   "metadata": {},
   "source": [
    "We investigated the data, checking for data unbalancing, visualizing the features and understanding the relationship between different features. We then investigated two predictive models. The data was split in 3 parts, a train set, a validation set and a test set. For the first three models, we only used the train and test set.\n",
    "\n",
    "We started with RandomForrestClassifier, for which we obtained an AUC scode of 0.85 when predicting the target for the test set.\n",
    "\n",
    "We then experimented with a LightGBM model. In this case, se used the validation set for validation of the training model. The best validation score obtained was 0.929.\n",
    "\n",
    "We then presented the data to a XGBoost model. We used both train-validation split and cross-validation to evaluate the model effectiveness to predict 'Class' value, i.e. detecting if a transaction was fraudulent. With the first method we obtained values of AUC for the validation set around 0.947980. For the test set, the score obtained was 0.903344.\n",
    "With the cross-validation, we obtained an AUC score for the test prediction of 0.947980."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e5659",
   "metadata": {},
   "source": [
    "Thanks to: https://www.kaggle.com/code/gpreda/credit-card-fraud-detection-predictive-models/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993aab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
